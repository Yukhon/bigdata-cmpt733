{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Entity Resolution (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists often spend 80% of their time on [data preparation](https://www.infoworld.com/article/3228245/the-80-20-data-science-dilemma.html). If your career goal is to become a data scientist, you have to master data cleaning and data integration skills. In this assignment, you will learn how to solve the Entity Resolution (ER) problem, a very common problem in data cleaning and integration. After completing this assignment, you should be able to answer the following questions:\n",
    "\n",
    "1. What is ER?\n",
    "2. What are the applications of ER in data integration and cleaning? \n",
    "3. How to avoid $n^2$ comparisons? \n",
    "4. How to compute Jaccard Similarity?\n",
    "5. How to evaluate an ER result?\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Please use [pandas.DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) rather than spark.DataFrame to manipulate data.\n",
    "\n",
    "2. Please follow python code style (https://www.python.org/dev/peps/pep-0008/). If TA finds your code hard to read, you will lose points. This requirement will stay for the whole semester.\n",
    "\n",
    "The data for Assignment 2 (Part 1 and Part 2) can be downloaded from [A2-data.zip](A2-data.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ER is defined as finding different records that refer to the same real-world entity, e.g., iPhone 4-th generation vs. iPhone four. It is central to data integration and cleaning. In this assignment, you will learn how to apply ER in a data integration setting. But the program that you are going to write can be easily extended to a data-cleaning setting, being used to detect _duplication values_.   \n",
    "\n",
    "Imagine that you want to help your company's customers to buy products at a cheaper price. In order to do so, you first write a [web scraper](https://nbviewer.jupyter.org/github/sfu-db/bigdata-cmpt733/blob/master/Assignments/A1/A1-1.ipynb) to crawl product data from Amazon.com and Google Shopping, respectively, and then integrate the data together. Since the same product may have different representations in the two websites, you are facing an ER problem. \n",
    "\n",
    "Existing ER techniques can be broadly divided into two categories: similarity-based (Part 1) and learning-based (Part 2). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike a learning-based technique, a similarity-based technique (a.k.a similarity join) does not need any label data. It first chooses a similarity function and a threshold, and then returns the record pairs whose similarity values are above the threshold. These returned record pairs are thought of as matching pairs, i.e., referring to the same real-world entity. \n",
    "\n",
    "Depending on particular applications, you may need to choose different similarity functions. In this assignment, we will use Jaccard similarity, i.e., $\\textsf{Jaccard}(r, s) = \\big|\\frac{r \\cap s}{r \\cup s}\\big|$. Here is the formal definition of this problem.\n",
    "\n",
    "> **Jaccard-Similarity Join**: Given two DataFrames, R and S, and a threshold $\\theta \\in (0, 1]$, the jaccard-similarity join problem aims to find all record pairs $(r, s) \\in R \\times S$ such that $\\textsf{Jaccard}(r, s) \\geq \\theta$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement similarity join, you need to address the following challenges:\n",
    "\n",
    "1. Jaccard is used to quantify the similarity between two sets instead of two records. You need to convert each record to a set.\n",
    "\n",
    "2. A naive implementation of similarity join is to compute Jaccard for all $|R \\times S|$ possible pairs. Imagine R and S have one million records. This requires doing 10^12 pair comparisons, which is extremely expensive. Thus, you need to know how to avoid n^2 comparisons. \n",
    "\n",
    "3. The output of ER is a set of matching pairs, where each pair is considered as referring to the same real-world entity. You need to know how to evaluate the quality of an ER result.\n",
    "\n",
    "Next, you will be guided to complete four tasks. After finishing these tasks, I suggest you going over the above challenges again, and understand how they are addressed.\n",
    "\n",
    "Read the code first, and then implement the remaining four functions: `preprocess_df`, `filtering`, `verification`, and `evaluate` by doing Tasks A-D, respectively.\n",
    "\n",
    "``` python\n",
    "# similarity_join.py\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class SimilarityJoin:\n",
    "    def __init__(self, data_file1, data_file2):\n",
    "        self.df1 = pd.read_csv(data_file1)\n",
    "        self.df2 = pd.read_csv(data_file2)\n",
    "          \n",
    "    def preprocess_df(self, df, cols): \n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\" \n",
    "    \n",
    "    def filtering(self, df1, df2):\n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\"\n",
    "      \n",
    "    def verification(self, cand_df, threshold):\n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\"\n",
    "        \n",
    "    def evaluate(self, result, ground_truth):\n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\"\n",
    "        \n",
    "    def jaccard_join(self, cols1, cols2, threshold):\n",
    "        new_df1 = self.preprocess_df(self.df1, cols1)\n",
    "        new_df2 = self.preprocess_df(self.df2, cols2)\n",
    "        print (\"Before filtering: %d pairs in total\" %(self.df1.shape[0] *self.df2.shape[0])) \n",
    "        \n",
    "        cand_df = self.filtering(new_df1, new_df2)\n",
    "        print (\"After Filtering: %d pairs left\" %(cand_df.shape[0]))\n",
    "        \n",
    "        result_df = self.verification(cand_df, threshold)\n",
    "        print (\"After Verification: %d similar pairs\" %(result_df.shape[0]))\n",
    "        \n",
    "        return result_df\n",
    "       \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    er = SimilarityJoin(\"Amazon_sample.csv\", \"Google_sample.csv\")\n",
    "    amazon_cols = [\"title\", \"manufacturer\"]\n",
    "    google_cols = [\"name\", \"manufacturer\"]\n",
    "    result_df = er.jaccard_join(amazon_cols, google_cols, 0.5)\n",
    "\n",
    "    result = result_df[['id1', 'id2']].values.tolist()\n",
    "    ground_truth = pd.read_csv(\"Amazon_Google_perfectMapping_sample.csv\").values.tolist()\n",
    "    print (\"(precision, recall, fmeasure) = \", er.evaluate(result, ground_truth))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_join.py\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class SimilarityJoin:\n",
    "    def __init__(self, data_file1, data_file2):\n",
    "        self.df1 = pd.read_csv(data_file1)\n",
    "        self.df2 = pd.read_csv(data_file2)\n",
    "          \n",
    "    def preprocess_df(self, df, cols): \n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\" \n",
    "    \n",
    "    def filtering(self, df1, df2):\n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\"\n",
    "      \n",
    "    def verification(self, cand_df, threshold):\n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\"\n",
    "        \n",
    "    def evaluate(self, result, ground_truth):\n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\"\n",
    "        \n",
    "    def jaccard_join(self, cols1, cols2, threshold):\n",
    "        new_df1 = self.preprocess_df(self.df1, cols1)\n",
    "        new_df2 = self.preprocess_df(self.df2, cols2)\n",
    "        print (\"Before filtering: %d pairs in total\" %(self.df1.shape[0] *self.df2.shape[0])) \n",
    "        \n",
    "        cand_df = self.filtering(new_df1, new_df2)\n",
    "        print (\"After Filtering: %d pairs left\" %(cand_df.shape[0]))\n",
    "        \n",
    "        result_df = self.verification(cand_df, threshold)\n",
    "        print (\"After Verification: %d similar pairs\" %(result_df.shape[0]))\n",
    "        \n",
    "        return result_df\n",
    "       \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    er = SimilarityJoin(\"Amazon_sample.csv\", \"Google_sample.csv\")\n",
    "    amazon_cols = [\"title\", \"manufacturer\"]\n",
    "    google_cols = [\"name\", \"manufacturer\"]\n",
    "    result_df = er.jaccard_join(amazon_cols, google_cols, 0.5)\n",
    "\n",
    "    result = result_df[['id1', 'id2']].values.tolist()\n",
    "    ground_truth = pd.read_csv(\"Amazon_Google_perfectMapping_sample.csv\").values.tolist()\n",
    "    print (\"(precision, recall, fmeasure) = \", er.evaluate(result, ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The program will output the following when running on the sample data:\n",
    "\n",
    "\n",
    "> Before filtering: 256 pairs in total\n",
    "\n",
    "> After Filtering: 84 pairs left\n",
    "\n",
    "> After Verification: 6 similar pairs\n",
    "\n",
    "> (precision, recall, fmeasure) =  (1.0, 0.375, 0.5454545454545454)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A. Data Preprocessing (Record --> Token Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Jaccard needs to take two sets as input, your first job is to preprocess DataFrames by transforming each record into a set of tokens. Please implement the following function.   \n",
    "\n",
    "```python\n",
    "def preprocess_df(self, df, cols): \n",
    "    \"\"\" \n",
    "        Input: $df represents a DataFrame\n",
    "               $cols represents the list of columns (in $df) that will be concatenated and be tokenized\n",
    "\n",
    "        Output: Return a new DataFrame that adds the \"joinKey\" column to the input $df\n",
    "\n",
    "        Comments: The \"joinKey\" column is a list of tokens, which is generated as follows:\n",
    "                 (1) concatenate the $cols in $df; \n",
    "                 (2) apply the tokenizer to the concatenated string\n",
    "        Here is how the tokenizer should work:\n",
    "                 (1) Use \"re.split(r'\\W+', string)\" to split a string into a set of tokens\n",
    "                 (2) Convert each token to its lower-case\n",
    "    \"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of testing, you can compare your outputs with new_df1 and new_df2 that can be found from the `Amazon-Google-Sample` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "      <th>joinKey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0002mg5uk</td>\n",
       "      <td>iview mediapro 2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>global marketing</td>\n",
       "      <td>199.99</td>\n",
       "      <td>[iview, mediapro, 2, 5, global, marketing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0002jtvng</td>\n",
       "      <td>bias deck le 3.5 macintosh cd</td>\n",
       "      <td>if you want to record music and audio like a p...</td>\n",
       "      <td>bias</td>\n",
       "      <td>99.00</td>\n",
       "      <td>[bias, deck, le, 3, 5, macintosh, cd, bias]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0007lw22g</td>\n",
       "      <td>apple ilife '06 (mac dvd) [older version]</td>\n",
       "      <td>ilife '06 is the easiest way to make the most ...</td>\n",
       "      <td>apple computer</td>\n",
       "      <td>79.00</td>\n",
       "      <td>[apple, ilife, 06, mac, dvd, older, version, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b00007kh02</td>\n",
       "      <td>extensis intellihance pro 4.x win/mac</td>\n",
       "      <td>extensis intellihance pro 4 quickly and dynami...</td>\n",
       "      <td>extensis corporation</td>\n",
       "      <td>199.99</td>\n",
       "      <td>[extensis, intellihance, pro, 4, x, win, mac, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b000saufpw</td>\n",
       "      <td>dk amazing animals 1.1</td>\n",
       "      <td>meet your cd host henry a delightful 3-d anima...</td>\n",
       "      <td>global software publishing</td>\n",
       "      <td>9.99</td>\n",
       "      <td>[dk, amazing, animals, 1, 1, global, software,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b000v7vz1u</td>\n",
       "      <td>onone essentials for adobe photoshop elements ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>onone software</td>\n",
       "      <td>59.95</td>\n",
       "      <td>[onone, essentials, for, adobe, photoshop, ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b00006hvvo</td>\n",
       "      <td>upg sgms 1000 incremental node</td>\n",
       "      <td>today enterprises and service providers face i...</td>\n",
       "      <td>sonic systems inc.</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[upg, sgms, 1000, incremental, node, sonic, sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b0000vyk1o</td>\n",
       "      <td>power director 3</td>\n",
       "      <td>powerdirector 3 - it's everything you need to ...</td>\n",
       "      <td>avanquest publishing usa inc.</td>\n",
       "      <td>79.95</td>\n",
       "      <td>[power, director, 3, avanquest, publishing, us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b000jx1kma</td>\n",
       "      <td>aircraft power pack for ms flight simulator</td>\n",
       "      <td>aircraft powerpack includes wings of power: he...</td>\n",
       "      <td>red mile</td>\n",
       "      <td>29.99</td>\n",
       "      <td>[aircraft, power, pack, for, ms, flight, simul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b000licg1m</td>\n",
       "      <td>power production storyboard artist 4</td>\n",
       "      <td>with storyboard artist 4 you can create profes...</td>\n",
       "      <td>power production</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[power, production, storyboard, artist, 4, pow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b00002s6sc</td>\n",
       "      <td>punch 5 in 1 home design</td>\n",
       "      <td>5 in 1 home design is a fully integrated 3-d h...</td>\n",
       "      <td>punch! software</td>\n",
       "      <td>39.99</td>\n",
       "      <td>[punch, 5, in, 1, home, design, punch, software]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b000gaqlxe</td>\n",
       "      <td>aquazone bass edition</td>\n",
       "      <td>aquazone bass edition mini box windows xp/w2k/...</td>\n",
       "      <td>smith micro software</td>\n",
       "      <td>19.99</td>\n",
       "      <td>[aquazone, bass, edition, smith, micro, software]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b00013wh0w</td>\n",
       "      <td>handmark oxford american desk dictionary and t...</td>\n",
       "      <td>the oxford american desk dictionary and thesau...</td>\n",
       "      <td>handmark inc.</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[handmark, oxford, american, desk, dictionary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b000ndibbo</td>\n",
       "      <td>adobe indesign cs3 upgrade from pagemaker [mac]</td>\n",
       "      <td>note: this is the upgrade from pagemaker versi...</td>\n",
       "      <td>adobe</td>\n",
       "      <td>199.00</td>\n",
       "      <td>[adobe, indesign, cs3, upgrade, from, pagemake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b000ndibge</td>\n",
       "      <td>adobe creative suite cs3 web standard [mac]</td>\n",
       "      <td>adobe creative suite 3 web standard software i...</td>\n",
       "      <td>adobe</td>\n",
       "      <td>999.00</td>\n",
       "      <td>[adobe, creative, suite, cs3, web, standard, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b000in8n30</td>\n",
       "      <td>hijack2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>navarre (software)</td>\n",
       "      <td>39.95</td>\n",
       "      <td>[hijack2, navarre, software, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "0   b0002mg5uk                                 iview mediapro 2.5   \n",
       "1   b0002jtvng                      bias deck le 3.5 macintosh cd   \n",
       "2   b0007lw22g          apple ilife '06 (mac dvd) [older version]   \n",
       "3   b00007kh02              extensis intellihance pro 4.x win/mac   \n",
       "4   b000saufpw                             dk amazing animals 1.1   \n",
       "5   b000v7vz1u  onone essentials for adobe photoshop elements ...   \n",
       "6   b00006hvvo                     upg sgms 1000 incremental node   \n",
       "7   b0000vyk1o                                   power director 3   \n",
       "8   b000jx1kma        aircraft power pack for ms flight simulator   \n",
       "9   b000licg1m               power production storyboard artist 4   \n",
       "10  b00002s6sc                           punch 5 in 1 home design   \n",
       "11  b000gaqlxe                              aquazone bass edition   \n",
       "12  b00013wh0w  handmark oxford american desk dictionary and t...   \n",
       "13  b000ndibbo    adobe indesign cs3 upgrade from pagemaker [mac]   \n",
       "14  b000ndibge        adobe creative suite cs3 web standard [mac]   \n",
       "15  b000in8n30                                            hijack2   \n",
       "\n",
       "                                          description  \\\n",
       "0                                                 NaN   \n",
       "1   if you want to record music and audio like a p...   \n",
       "2   ilife '06 is the easiest way to make the most ...   \n",
       "3   extensis intellihance pro 4 quickly and dynami...   \n",
       "4   meet your cd host henry a delightful 3-d anima...   \n",
       "5                                                 NaN   \n",
       "6   today enterprises and service providers face i...   \n",
       "7   powerdirector 3 - it's everything you need to ...   \n",
       "8   aircraft powerpack includes wings of power: he...   \n",
       "9   with storyboard artist 4 you can create profes...   \n",
       "10  5 in 1 home design is a fully integrated 3-d h...   \n",
       "11  aquazone bass edition mini box windows xp/w2k/...   \n",
       "12  the oxford american desk dictionary and thesau...   \n",
       "13  note: this is the upgrade from pagemaker versi...   \n",
       "14  adobe creative suite 3 web standard software i...   \n",
       "15                                                NaN   \n",
       "\n",
       "                     manufacturer   price  \\\n",
       "0                global marketing  199.99   \n",
       "1                            bias   99.00   \n",
       "2                  apple computer   79.00   \n",
       "3            extensis corporation  199.99   \n",
       "4      global software publishing    9.99   \n",
       "5                  onone software   59.95   \n",
       "6              sonic systems inc.    0.00   \n",
       "7   avanquest publishing usa inc.   79.95   \n",
       "8                        red mile   29.99   \n",
       "9                power production    0.00   \n",
       "10                punch! software   39.99   \n",
       "11           smith micro software   19.99   \n",
       "12                  handmark inc.    0.00   \n",
       "13                          adobe  199.00   \n",
       "14                          adobe  999.00   \n",
       "15             navarre (software)   39.95   \n",
       "\n",
       "                                              joinKey  \n",
       "0          [iview, mediapro, 2, 5, global, marketing]  \n",
       "1         [bias, deck, le, 3, 5, macintosh, cd, bias]  \n",
       "2   [apple, ilife, 06, mac, dvd, older, version, a...  \n",
       "3   [extensis, intellihance, pro, 4, x, win, mac, ...  \n",
       "4   [dk, amazing, animals, 1, 1, global, software,...  \n",
       "5   [onone, essentials, for, adobe, photoshop, ele...  \n",
       "6   [upg, sgms, 1000, incremental, node, sonic, sy...  \n",
       "7   [power, director, 3, avanquest, publishing, us...  \n",
       "8   [aircraft, power, pack, for, ms, flight, simul...  \n",
       "9   [power, production, storyboard, artist, 4, pow...  \n",
       "10   [punch, 5, in, 1, home, design, punch, software]  \n",
       "11  [aquazone, bass, edition, smith, micro, software]  \n",
       "12  [handmark, oxford, american, desk, dictionary,...  \n",
       "13  [adobe, indesign, cs3, upgrade, from, pagemake...  \n",
       "14  [adobe, creative, suite, cs3, web, standard, m...  \n",
       "15                     [hijack2, navarre, software, ]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def preprocess_df(df, cols): \n",
    "    \"\"\" \n",
    "        Input: $df represents a DataFrame\n",
    "               $cols represents the list of columns (in $df) that will be concatenated and be tokenized\n",
    "\n",
    "        Output: Return a new DataFrame that adds the \"joinKey\" column to the input $df\n",
    "\n",
    "        Comments: The \"joinKey\" column is a list of tokens, which is generated as follows:\n",
    "                 (1) concatenate the $cols in $df; \n",
    "                 (2) apply the tokenizer to the concatenated string\n",
    "        Here is how the tokenizer should work:\n",
    "                 (1) Use \"re.split(r'\\W+', string)\" to split a string into a set of tokens\n",
    "                 (2) Convert each token to its lower-case\n",
    "    \"\"\" \n",
    "    df['joinKey'] = \"\"\n",
    "    df['joinKey'] = df[cols].apply(lambda x: x.str.cat(sep=' '), axis=1)\n",
    "    df[\"joinKey\"] = df[\"joinKey\"].str.lower()\n",
    "    df[\"joinKey\"] = df[\"joinKey\"].str.split(r'\\W+')\n",
    "    return df\n",
    "amazon_cols = [\"title\", \"manufacturer\"]\n",
    "google_cols = [\"name\", \"manufacturer\"]\n",
    "df1 = pd.read_csv('A2-data/part1-similarity-join/Amazon-Google-Sample/Amazon_sample.csv')\n",
    "df2 = pd.read_csv('A2-data/part1-similarity-join/Amazon-Google-Sample/Google_sample.csv')\n",
    "df1= preprocess_df(df1, amazon_cols)\n",
    "df2 = preprocess_df(df2, google_cols)\n",
    "\n",
    "df1[\"joinKey\"].to_csv('data1.csv')\n",
    "df2[\"joinKey\"].to_csv('data2.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B. Filtering Obviously Non-matching Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid $n^2$ pair comparisons, ER algorithms often follow a filtering-and-verification framework. The basic idea is to first filter obviously non-matching pairs and then only verify the remaining pairs.  \n",
    "\n",
    "In Task B, your job is to implement the <font color=\"blue\">filtering</font> function. This function will filter all the record pairs whose joinKeys do not share any token. This is because based on the definition of Jaccard, we can deduce that **if two sets do not share any element (i.e., $r\\cap s = \\phi$), their Jaccard similarity values must be zero**. Thus, we can safely remove them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def filtering(self, df1, df2):\n",
    "    \"\"\" \n",
    "        Input: $df1 and $df2 are two input DataFrames, where each of them \n",
    "               has a 'joinKey' column added by the preprocess_df function\n",
    "\n",
    "        Output: Return a new DataFrame $cand_df with four columns: 'id1', 'joinKey1', 'id2', 'joinKey2',\n",
    "                where 'id1' and 'joinKey1' are from $df1, and 'id2' and 'joinKey2'are from $df2.\n",
    "                Intuitively, $cand_df is the joined result between $df1 and $df2 on the condition that \n",
    "                their joinKeys share at least one token. \n",
    "\n",
    "        Comments: Since the goal of the \"filtering\" function is to avoid n^2 pair comparisons, \n",
    "                  you are NOT allowed to compute a cartesian join between $df1 and $df2 in the function. \n",
    "                  Please come up with a more efficient algorithm (see hints in Lecture 2). \n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>joinKey1</th>\n",
       "      <th>joinKey2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0002mg5uk</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1267...</td>\n",
       "      <td>[iview, mediapro, 2, 5, global, marketing]</td>\n",
       "      <td>[iview, mediapro, 2, 6, media, management]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b0002mg5uk</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1761...</td>\n",
       "      <td>[iview, mediapro, 2, 5, global, marketing]</td>\n",
       "      <td>[onone, software, essentials, for, adobe, phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b000v7vz1u</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1267...</td>\n",
       "      <td>[onone, essentials, for, adobe, photoshop, ele...</td>\n",
       "      <td>[iview, mediapro, 2, 6, media, management]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b000v7vz1u</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1761...</td>\n",
       "      <td>[onone, essentials, for, adobe, photoshop, ele...</td>\n",
       "      <td>[onone, software, essentials, for, adobe, phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b0002mg5uk</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1102...</td>\n",
       "      <td>[iview, mediapro, 2, 5, global, marketing]</td>\n",
       "      <td>[punch, software, 20100, punch, 5, in, 1, home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>b00013wh0w</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1020...</td>\n",
       "      <td>[handmark, oxford, american, desk, dictionary,...</td>\n",
       "      <td>[palmspring, software, 523, oxford, american, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>b00013wh0w</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/7249...</td>\n",
       "      <td>[handmark, oxford, american, desk, dictionary,...</td>\n",
       "      <td>[onone, software, ice, 10770, intellihance, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>b00013wh0w</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/4377...</td>\n",
       "      <td>[handmark, oxford, american, desk, dictionary,...</td>\n",
       "      <td>[hijack2, identity, and, data, security, suite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>b000ndibge</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/4377...</td>\n",
       "      <td>[adobe, creative, suite, cs3, web, standard, m...</td>\n",
       "      <td>[hijack2, identity, and, data, security, suite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>b000in8n30</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/4377...</td>\n",
       "      <td>[hijack2, navarre, software, ]</td>\n",
       "      <td>[hijack2, identity, and, data, security, suite]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id1                                                id2  \\\n",
       "0    b0002mg5uk  http://www.google.com/base/feeds/snippets/1267...   \n",
       "3    b0002mg5uk  http://www.google.com/base/feeds/snippets/1761...   \n",
       "4    b000v7vz1u  http://www.google.com/base/feeds/snippets/1267...   \n",
       "5    b000v7vz1u  http://www.google.com/base/feeds/snippets/1761...   \n",
       "6    b0002mg5uk  http://www.google.com/base/feeds/snippets/1102...   \n",
       "..          ...                                                ...   \n",
       "195  b00013wh0w  http://www.google.com/base/feeds/snippets/1020...   \n",
       "199  b00013wh0w  http://www.google.com/base/feeds/snippets/7249...   \n",
       "200  b00013wh0w  http://www.google.com/base/feeds/snippets/4377...   \n",
       "211  b000ndibge  http://www.google.com/base/feeds/snippets/4377...   \n",
       "214  b000in8n30  http://www.google.com/base/feeds/snippets/4377...   \n",
       "\n",
       "                                              joinKey1  \\\n",
       "0           [iview, mediapro, 2, 5, global, marketing]   \n",
       "3           [iview, mediapro, 2, 5, global, marketing]   \n",
       "4    [onone, essentials, for, adobe, photoshop, ele...   \n",
       "5    [onone, essentials, for, adobe, photoshop, ele...   \n",
       "6           [iview, mediapro, 2, 5, global, marketing]   \n",
       "..                                                 ...   \n",
       "195  [handmark, oxford, american, desk, dictionary,...   \n",
       "199  [handmark, oxford, american, desk, dictionary,...   \n",
       "200  [handmark, oxford, american, desk, dictionary,...   \n",
       "211  [adobe, creative, suite, cs3, web, standard, m...   \n",
       "214                     [hijack2, navarre, software, ]   \n",
       "\n",
       "                                              joinKey2  \n",
       "0           [iview, mediapro, 2, 6, media, management]  \n",
       "3    [onone, software, essentials, for, adobe, phot...  \n",
       "4           [iview, mediapro, 2, 6, media, management]  \n",
       "5    [onone, software, essentials, for, adobe, phot...  \n",
       "6    [punch, software, 20100, punch, 5, in, 1, home...  \n",
       "..                                                 ...  \n",
       "195  [palmspring, software, 523, oxford, american, ...  \n",
       "199  [onone, software, ice, 10770, intellihance, pr...  \n",
       "200    [hijack2, identity, and, data, security, suite]  \n",
       "211    [hijack2, identity, and, data, security, suite]  \n",
       "214    [hijack2, identity, and, data, security, suite]  \n",
       "\n",
       "[84 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filtering(df1, df2):\n",
    "    \"\"\" \n",
    "        Input: $df1 and $df2 are two input DataFrames, where each of them \n",
    "               has a 'joinKey' column added by the preprocess_df function\n",
    "\n",
    "        Output: Return a new DataFrame $cand_df with four columns: 'id1', 'joinKey1', 'id2', 'joinKey2',\n",
    "                where 'id1' and 'joinKey1' are from $df1, and 'id2' and 'joinKey2'are from $df2.\n",
    "                Intuitively, $cand_df is the joined result between $df1 and $df2 on the condition that \n",
    "                their joinKeys share at least one token. \n",
    "\n",
    "        Comments: Since the goal of the \"filtering\" function is to avoid n^2 pair comparisons, \n",
    "                  you are NOT allowed to compute a cartesian join between $df1 and $df2 in the function. \n",
    "                  Please come up with a more efficient algorithm (see hints in Lecture 2). \n",
    "    \"\"\"\n",
    "    # every df flatten, then df.join with same key\n",
    "    # remove duplicate combinations (id),then get back each corresponding content\n",
    "    df1['joinKey1'] = df1['joinKey']\n",
    "    df2['joinKey2'] = df2['joinKey']\n",
    "    df1_explode = df1.explode('joinKey')\n",
    "    df2_explote = df2.explode('joinKey')\n",
    "    df_result = df1_explode.merge(df2_explote, on='joinKey')[['id_x','id_y','joinKey1', 'joinKey2']].rename(columns={\"id_x\":\"id1\", \"id_y\":\"id2\"})\n",
    "    df_result = df_result.drop_duplicates(['id1','id2'])\n",
    "    df_result.drop(df_result.columns[df_result.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "    return df_result\n",
    "df1['joinKey1'] = df1['joinKey']\n",
    "df2['joinKey2'] = df2['joinKey']\n",
    "df1_explode = df1.explode('joinKey')\n",
    "df2_explote = df2.explode('joinKey')\n",
    "df_result = df1_explode.merge(df2_explote, on='joinKey')[['id_x','id_y','joinKey1', 'joinKey2']].rename(columns={\"id_x\":\"id1\", \"id_y\":\"id2\"})\n",
    "df_result = df_result.drop_duplicates(['id1','id2'])\n",
    "df_result.drop(df_result.columns[df_result.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "df_result\n",
    "cand_df = filtering(df1,df2)\n",
    "cand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of testing, you can compare your output with cand_df that can be found from the `Amazon-Google-Sample` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C. Computing Jaccard Similarity for Survived Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second phase of the filtering-and-verification framework, we will compute the Jaccard similarity for each survived pair and return those pairs whose jaccard similarity values are no smaller than the specified threshold.\n",
    "\n",
    "In Task C, your job is to implement the <font color=\"blue\">verification</font> function. This task looks simple, but there are a few small \"traps\". \n",
    "\n",
    "\n",
    "```python\n",
    "def verification(self, cand_df, threshold):\n",
    "        \"\"\" \n",
    "            Input: $cand_df is the output DataFrame from the 'filtering' function. \n",
    "                   $threshold is a float value between (0, 1] \n",
    "\n",
    "            Output: Return a new DataFrame $result_df that represents the ER result. \n",
    "                    It has five columns: id1, joinKey1, id2, joinKey2, jaccard \n",
    "\n",
    "            Comments: There are two differences between $cand_df and $result_df\n",
    "                      (1) $result_df adds a new column, called jaccard, which stores the jaccard similarity \n",
    "                          between $joinKey1 and $joinKey2\n",
    "                      (2) $result_df removes the rows whose jaccard similarity is smaller than $threshold \n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>joinKey1</th>\n",
       "      <th>joinKey2</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b000v7vz1u</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1761...</td>\n",
       "      <td>[onone, essentials, for, adobe, photoshop, ele...</td>\n",
       "      <td>[onone, software, essentials, for, adobe, phot...</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b00002s6sc</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1102...</td>\n",
       "      <td>[punch, 5, in, 1, home, design, punch, software]</td>\n",
       "      <td>[punch, software, 20100, punch, 5, in, 1, home...</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>b000ndibbo</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1693...</td>\n",
       "      <td>[adobe, indesign, cs3, upgrade, from, pagemake...</td>\n",
       "      <td>[adobe, indesign, cs3, for, mac, upgrade, from...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>b000jx1kma</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1835...</td>\n",
       "      <td>[aircraft, power, pack, for, ms, flight, simul...</td>\n",
       "      <td>[red, mile, entertainment, 124, aircraft, powe...</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>b000ndibge</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/1227...</td>\n",
       "      <td>[adobe, creative, suite, cs3, web, standard, m...</td>\n",
       "      <td>[adobe, cs3, web, standard]</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>b000licg1m</td>\n",
       "      <td>http://www.google.com/base/feeds/snippets/6070...</td>\n",
       "      <td>[power, production, storyboard, artist, 4, pow...</td>\n",
       "      <td>[power, production, power, production, storybo...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id1                                                id2  \\\n",
       "5    b000v7vz1u  http://www.google.com/base/feeds/snippets/1761...   \n",
       "10   b00002s6sc  http://www.google.com/base/feeds/snippets/1102...   \n",
       "31   b000ndibbo  http://www.google.com/base/feeds/snippets/1693...   \n",
       "128  b000jx1kma  http://www.google.com/base/feeds/snippets/1835...   \n",
       "140  b000ndibge  http://www.google.com/base/feeds/snippets/1227...   \n",
       "161  b000licg1m  http://www.google.com/base/feeds/snippets/6070...   \n",
       "\n",
       "                                              joinKey1  \\\n",
       "5    [onone, essentials, for, adobe, photoshop, ele...   \n",
       "10    [punch, 5, in, 1, home, design, punch, software]   \n",
       "31   [adobe, indesign, cs3, upgrade, from, pagemake...   \n",
       "128  [aircraft, power, pack, for, ms, flight, simul...   \n",
       "140  [adobe, creative, suite, cs3, web, standard, m...   \n",
       "161  [power, production, storyboard, artist, 4, pow...   \n",
       "\n",
       "                                              joinKey2   jaccard  \n",
       "5    [onone, software, essentials, for, adobe, phot...  0.909091  \n",
       "10   [punch, software, 20100, punch, 5, in, 1, home...  0.727273  \n",
       "31   [adobe, indesign, cs3, for, mac, upgrade, from...  1.000000  \n",
       "128  [red, mile, entertainment, 124, aircraft, powe...  0.529412  \n",
       "140                        [adobe, cs3, web, standard]  0.500000  \n",
       "161  [power, production, power, production, storybo...  0.600000  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def verification(cand_df, threshold):\n",
    "        \"\"\" \n",
    "            Input: $cand_df is the output DataFrame from the 'filtering' function. \n",
    "                   $threshold is a float value between (0, 1] \n",
    "\n",
    "            Output: Return a new DataFrame $result_df that represents the ER result. \n",
    "                    It has five columns: id1, joinKey1, id2, joinKey2, jaccard \n",
    "\n",
    "            Comments: There are two differences between $cand_df and $result_df\n",
    "                      (1) $result_df adds a new column, called jaccard, which stores the jaccard similarity \n",
    "                          between $joinKey1 and $joinKey2\n",
    "                      (2) $result_df removes the rows whose jaccard similarity is smaller than $threshold \n",
    "        \"\"\"\n",
    "        cand_df['jaccard_abslength']  = cand_df.apply(lambda x: len(x.joinKey1) + len(x.joinKey2), axis=1)\n",
    "        cand_df['inter'] = cand_df.apply(lambda x : len([value for value in x.joinKey1 if value in x.joinKey2]), axis=1)\n",
    "        cand_df['inter'] = cand_df.apply(lambda x : x.inter if x.inter < min(len(x.joinKey1), len(x.joinKey2)) else min( len(x.joinKey1), len(x.joinKey2)), axis=1)\n",
    "        cand_df['jaccard'] = cand_df['inter']/ (cand_df['jaccard_abslength'] - cand_df['inter'])\n",
    "        cand_df = cand_df[cand_df['jaccard']>=threshold]\n",
    "        cand_df = cand_df[['id1','id2', 'joinKey1', 'joinKey2', 'jaccard']]\n",
    "        return cand_df\n",
    "\n",
    "\n",
    "# cand_df['jaccard_abslength']  = cand_df.apply(lambda x: len(x.joinKey1) + len(x.joinKey2), axis=1)\n",
    "# cand_df['inter'] = cand_df.apply(lambda x : len([value for value in x.joinKey1 if value in x.joinKey2]), axis=1)\n",
    "# cand_df['inter'] = cand_df.apply(lambda x : x.inter if x.inter < min(len(x.joinKey1), len(x.joinKey2)) else min( len(x.joinKey1), len(x.joinKey2)), axis=1)\n",
    "# cand_df['jaccard'] = cand_df['inter']/ (cand_df['jaccard_abslength'] - cand_df['inter'])\n",
    "# cand_df = cand_df[cand_df['jaccard']>0.5]\n",
    "# cand_df\n",
    "# cand_df = cand_df[['id1','id2', 'joinKey1', 'joinKey2', 'jaccard']]\n",
    "# cand_df\n",
    "cand_df = verification(cand_df, 0.5)\n",
    "cand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of testing, you can compare your output with result_df that can be found from the `Amazon-Google-Sample` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D. Evaluating an ER result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we evaluate an ER result? Before answering this question, let's first recall what the ER result looks like. The goal of ER is to identify all matching record pairs. Thus, the ER result should be a set of identified matching pairs, denoted by R. One thing that we want to know is that what percentage of the pairs in $R$ that are truly matching? This is what Precision can tell us. Let $T$ denote the truly matching pairs in $R$. Precision is defined as:\n",
    "$$Precision = \\frac{|T|}{|R|}$$\n",
    "\n",
    "In addition to Precision, another thing that we care about is that what percentage of truly matching pairs that are identified. This is what Recall can tell us. Let $A$ denote the truly matching pairs in the entire dataset. Recall is defined as: \n",
    "\n",
    "$$Recall = \\frac{|T|}{|A|}$$\n",
    "\n",
    "There is an interesting trade-off between Precision and Recall. As more and more pairs that are identified as matching, Recall increases while Precision potentially decreases. For the extreme case, if we return all the pairs as matching pairs, we will get a perfect Recall (i.e., Recall = 100%) but precision will be the worst. Thus, to balance Precision and Recall, people often use FMeasure to evaluate an ER result:\n",
    "\n",
    "$$FMeasure = \\frac{2*Precision*Recall}{Precision+Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task D, you will be given an ER result as well as the ground truth that tells you what pairs are truly matching. Your job is to calculate Precision, Recall and FMeasure for the result. \n",
    "```python\n",
    "def evaluate(self, result, ground_truth):\n",
    "    \"\"\" \n",
    "        Input: $result is a list of matching pairs identified by the ER algorithm\n",
    "               $ground_truth is a list of matching pairs labeld by humans\n",
    "\n",
    "        Output: Compute precision, recall, and fmeasure of $result based on $ground_truth, and\n",
    "                return the evaluation result as a triple: (precision, recall, fmeasure)\n",
    "\n",
    "    \"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(result, ground_truth):\n",
    "    \"\"\" \n",
    "        Input: $result is a list of matching pairs identified by the ER algorithm\n",
    "               $ground_truth is a list of matching pairs labeld by humans\n",
    "\n",
    "        Output: Compute precision, recall, and fmeasure of $result based on $ground_truth, and\n",
    "                return the evaluation result as a triple: (precision, recall, fmeasure)\n",
    "\n",
    "    \"\"\"\n",
    "    precision = len([value for value in result if value in ground_truth])/len(result)\n",
    "    recall =  len([value for value in result if value in ground_truth])/len(ground_truth)\n",
    "    FMeasure = 2*precision*recall / (precision+recall)\n",
    "    return precision, recall, FMeasure\n",
    "result = cand_df[['id1', 'id2']].values.tolist()\n",
    "ground_truth = pd.read_csv(\"A2-data/part1-similarity-join/Amazon-Google-Sample/Amazon_Google_perfectMapping_sample.csv\").values.tolist()\n",
    "res1, res2, res3 = evaluate(result,ground_truth)\n",
    "res3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 256 pairs in total\n",
      "After Filtering: 84 pairs left\n",
      "After Verification: 6 similar pairs\n",
      "(precision, recall, fmeasure) =  (1.0, 0.375, 0.5454545454545454)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class SimilarityJoin:\n",
    "    def __init__(self, data_file1, data_file2):\n",
    "        self.df1 = pd.read_csv(data_file1)\n",
    "        self.df2 = pd.read_csv(data_file2)\n",
    "          \n",
    "    def preprocess_df(self, df, cols): \n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\" \n",
    "        df['joinKey'] = \"\"\n",
    "        df['joinKey'] = df[cols].apply(lambda x: x.str.cat(sep=' '), axis=1)\n",
    "        df[\"joinKey\"] = df[\"joinKey\"].str.lower()\n",
    "        df[\"joinKey\"] = df[\"joinKey\"].str.split(r'\\W+')\n",
    "        return df\n",
    "    \n",
    "    def filtering(self, df1, df2):\n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\"\n",
    "        df1['joinKey1'] = df1['joinKey']\n",
    "        df2['joinKey2'] = df2['joinKey']\n",
    "        df1_explode = df1.explode('joinKey')\n",
    "        df2_explote = df2.explode('joinKey')\n",
    "        df_result = df1_explode.merge(df2_explote, on='joinKey')[['id_x','id_y','joinKey1', 'joinKey2']].rename(columns={\"id_x\":\"id1\", \"id_y\":\"id2\"})\n",
    "        df_result = df_result.drop_duplicates(['id1','id2'])\n",
    "        df_result.drop(df_result.columns[df_result.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "        return df_result\n",
    "      \n",
    "    def verification(self, cand_df, threshold):\n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\"\n",
    "        cand_df['jaccard_abslength']  = cand_df.apply(lambda x: len(x.joinKey1) + len(x.joinKey2), axis=1)\n",
    "        cand_df['inter'] = cand_df.apply(lambda x : len([value for value in x.joinKey1 if value in x.joinKey2]), axis=1)\n",
    "        cand_df['inter'] = cand_df.apply(lambda x : x.inter if x.inter < min(len(x.joinKey1), len(x.joinKey2)) else min( len(x.joinKey1), len(x.joinKey2)), axis=1)\n",
    "        cand_df['jaccard'] = cand_df['inter']/ (cand_df['jaccard_abslength'] - cand_df['inter'])\n",
    "        cand_df = cand_df[cand_df['jaccard']>=threshold]\n",
    "        cand_df = cand_df[['id1','id2', 'joinKey1', 'joinKey2', 'jaccard']]\n",
    "        return cand_df\n",
    "        \n",
    "    def evaluate(self, result, ground_truth):\n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\"\n",
    "        precision = len([value for value in result if value in ground_truth])/len(result)\n",
    "        recall =  len([value for value in result if value in ground_truth])/len(ground_truth)\n",
    "        FMeasure = 2*precision*recall / (precision+recall)\n",
    "        return precision, recall, FMeasure\n",
    "        \n",
    "    def jaccard_join(self, cols1, cols2, threshold):\n",
    "        new_df1 = self.preprocess_df(self.df1, cols1)\n",
    "        new_df2 = self.preprocess_df(self.df2, cols2)\n",
    "        print (\"Before filtering: %d pairs in total\" %(self.df1.shape[0] *self.df2.shape[0])) \n",
    "        \n",
    "        cand_df = self.filtering(new_df1, new_df2)\n",
    "        print (\"After Filtering: %d pairs left\" %(cand_df.shape[0]))\n",
    "        \n",
    "        result_df = self.verification(cand_df, threshold)\n",
    "        print (\"After Verification: %d similar pairs\" %(result_df.shape[0]))\n",
    "        \n",
    "        return result_df\n",
    "       \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # er = SimilarityJoin(\"Amazon_sample.csv\", \"Google_sample.csv\")\n",
    "    er = SimilarityJoin(\"A2-data/part1-similarity-join/Amazon-Google-Sample/Amazon_sample.csv\",\"A2-data/part1-similarity-join/Amazon-Google-Sample/Google_sample.csv\")\n",
    "    amazon_cols = [\"title\", \"manufacturer\"]\n",
    "    google_cols = [\"name\", \"manufacturer\"]\n",
    "    result_df = er.jaccard_join(amazon_cols, google_cols, 0.5)\n",
    "\n",
    "    result = result_df[['id1', 'id2']].values.tolist()\n",
    "    ground_truth = pd.read_csv(\"A2-data/part1-similarity-join/Amazon-Google-Sample/Amazon_Google_perfectMapping_sample.csv\").values.tolist()\n",
    "    print (\"(precision, recall, fmeasure) = \", er.evaluate(result, ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `preprocess_df`, `filtering`, `verification`, and `evaluate` functions in `similarity_join.py`. Submit your code file (`similarity_join.py`)  to the CourSys activity Assignment 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
